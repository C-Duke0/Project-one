{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb6f85bc-1f35-4080-a59f-dfc9d1f81e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 342 videos for the year 2013\n",
      "Fetched 230 videos for the year 2014\n",
      "Fetched 267 videos for the year 2015\n",
      "Fetched 221 videos for the year 2016\n",
      "Fetched 232 videos for the year 2017\n",
      "Fetched 217 videos for the year 2018\n",
      "Fetched 241 videos for the year 2019\n",
      "Fetched 207 videos for the year 2020\n",
      "Fetched 151 videos for the year 2021\n",
      "Fetched 87 videos for the year 2022\n",
      "Fetched 114 videos for the year 2023\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "# Replace 'YOUR_API_KEY' with your actual API key.\n",
    "api_key = 'Your Passoword Here'\n",
    "\n",
    "# Create the YouTube service using the API key\n",
    "youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "def get_videos_from_year(year, num_videos):\n",
    "    # Calculate the date range for the given year\n",
    "    published_after = f'{year}-01-01T00:00:00Z'\n",
    "    published_before = f'{year + 1}-01-01T00:00:00Z'\n",
    "    \n",
    "    # Initialize a list to store the videos\n",
    "    videos = []\n",
    "\n",
    "    # Keep track of the next page token for pagination\n",
    "    next_page_token = None\n",
    "\n",
    "    # Retrieve up to `num_videos` videos from the given year\n",
    "    while len(videos) < num_videos:\n",
    "        # Use the search.list() method to search for videos within the date range\n",
    "        search_request = youtube.search().list(\n",
    "            part='id,snippet',\n",
    "            type='video',\n",
    "            publishedAfter=published_after,\n",
    "            publishedBefore=published_before,\n",
    "            maxResults=50,  # Maximum number of results per request (up to 50)\n",
    "            pageToken=next_page_token  # Use the next page token for pagination\n",
    "        )\n",
    "        \n",
    "        # Execute the search request and get the response\n",
    "        search_response = search_request.execute()\n",
    "        \n",
    "        # Extract video IDs and publication dates from the search results\n",
    "        video_ids = []\n",
    "        for item in search_response['items']:\n",
    "            video_id = item['id']['videoId']\n",
    "            published_at = item['snippet']['publishedAt']\n",
    "            videos.append({'id': video_id, 'published_at': published_at})\n",
    "            video_ids.append(video_id)\n",
    "        \n",
    "        # Use the videos.list() method to retrieve details about the videos\n",
    "        if video_ids:\n",
    "            video_request = youtube.videos().list(\n",
    "                part='contentDetails,statistics',\n",
    "                id=','.join(video_ids)\n",
    "            )\n",
    "            \n",
    "            # Execute the video request and get the response\n",
    "            video_response = video_request.execute()\n",
    "            \n",
    "            # Add length and views to the videos list\n",
    "            for video in video_response['items']:\n",
    "                # Find the video in the list by ID and update its information\n",
    "                for video_info in videos:\n",
    "                    if video_info['id'] == video['id']:\n",
    "                        video_info['length'] = video['contentDetails']['duration']\n",
    "                        video_info['views'] = video['statistics'].get('viewCount', 'N/A')\n",
    "        \n",
    "        # Get the next page token for pagination\n",
    "        next_page_token = search_response.get('nextPageToken')\n",
    "        \n",
    "        # If there are no more pages, break out of the loop\n",
    "        if not next_page_token:\n",
    "            break\n",
    "    \n",
    "    # Return the list of videos (limited to `num_videos`)\n",
    "    return videos[:num_videos]\n",
    "\n",
    "# Define the range of years you want to retrieve videos from\n",
    "years = range(2013, 2024)\n",
    "\n",
    "# Initialize a dictionary to store the DataFrames for each year\n",
    "videos_by_year_df = {}\n",
    "\n",
    "# Retrieve 500 videos from each year in the specified range and convert to DataFrame\n",
    "for year in years:\n",
    "    videos = get_videos_from_year(year, 500)\n",
    "    # Convert the list of videos to a pandas DataFrame\n",
    "    df = pd.DataFrame(videos)\n",
    "    # Store the DataFrame in the dictionary\n",
    "    videos_by_year_df[year] = df\n",
    "    print(f'Fetched {len(df)} videos for the year {year}')\n",
    "\n",
    "# Output the dictionary of DataFrames\n",
    "videos_by_year_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68a7b677-ecd8-467b-80e9-1fac2b10a4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 342 videos for the year 2013\n",
      "DataFrame for year 2013 exported to videos_2013.csv\n",
      "Fetched 238 videos for the year 2014\n",
      "DataFrame for year 2014 exported to videos_2014.csv\n",
      "Fetched 267 videos for the year 2015\n",
      "DataFrame for year 2015 exported to videos_2015.csv\n",
      "Fetched 221 videos for the year 2016\n",
      "DataFrame for year 2016 exported to videos_2016.csv\n",
      "Fetched 233 videos for the year 2017\n",
      "DataFrame for year 2017 exported to videos_2017.csv\n",
      "Fetched 233 videos for the year 2018\n",
      "DataFrame for year 2018 exported to videos_2018.csv\n",
      "Fetched 249 videos for the year 2019\n",
      "DataFrame for year 2019 exported to videos_2019.csv\n",
      "Fetched 231 videos for the year 2020\n",
      "DataFrame for year 2020 exported to videos_2020.csv\n",
      "Fetched 154 videos for the year 2021\n",
      "DataFrame for year 2021 exported to videos_2021.csv\n",
      "Fetched 150 videos for the year 2022\n",
      "DataFrame for year 2022 exported to videos_2022.csv\n",
      "Fetched 132 videos for the year 2023\n",
      "DataFrame for year 2023 exported to videos_2023.csv\n"
     ]
    }
   ],
   "source": [
    "for year in years:\n",
    "    videos = get_videos_from_year(year, 500)\n",
    "    # Convert the list of videos to a pandas DataFrame\n",
    "    df = pd.DataFrame(videos)\n",
    "    # Store the DataFrame in the dictionary\n",
    "    videos_by_year_df[year] = df\n",
    "    print(f'Fetched {len(df)} videos for the year {year}')\n",
    "    \n",
    "    # Export the DataFrame to a CSV file\n",
    "    file_path = f'videos_{year}.csv'\n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(f'DataFrame for year {year} exported to {file_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df93f974-90ab-41c3-b77f-d578779ec796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 350 videos for the year 2013\n",
      "Fetched 230 videos for the year 2014\n",
      "Fetched 267 videos for the year 2015\n",
      "Fetched 215 videos for the year 2016\n",
      "Fetched 232 videos for the year 2017\n",
      "Fetched 224 videos for the year 2018\n",
      "Fetched 241 videos for the year 2019\n",
      "Fetched 207 videos for the year 2020\n"
     ]
    },
    {
     "ename": "HttpError",
     "evalue": "<HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/search?part=id%2Csnippet&type=video&publishedAfter=2021-01-01T00%3A00%3A00Z&publishedBefore=2022-01-01T00%3A00%3A00Z&maxResults=50&pageToken=CGQQAA&key=AIzaSyCPsdodulfCK_q0DtSoTbq6ybocTWJkklg&alt=json returned \"The request cannot be completed because you have exceeded your <a href=\"/youtube/v3/getting-started#quota\">quota</a>.\". Details: \"[{'message': 'The request cannot be completed because you have exceeded your <a href=\"/youtube/v3/getting-started#quota\">quota</a>.', 'domain': 'youtube.quota', 'reason': 'quotaExceeded'}]\">",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHttpError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Retrieve 500 videos from each year in the specified range and convert to DataFrame\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m year \u001b[38;5;129;01min\u001b[39;00m years:\n\u001b[0;32m----> 5\u001b[0m     videos \u001b[38;5;241m=\u001b[39m \u001b[43mget_videos_from_year\u001b[49m\u001b[43m(\u001b[49m\u001b[43myear\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# Convert the list of videos to a pandas DataFrame\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(videos)\n",
      "Cell \u001b[0;32mIn[12], line 34\u001b[0m, in \u001b[0;36mget_videos_from_year\u001b[0;34m(year, num_videos)\u001b[0m\n\u001b[1;32m     24\u001b[0m search_request \u001b[38;5;241m=\u001b[39m youtube\u001b[38;5;241m.\u001b[39msearch()\u001b[38;5;241m.\u001b[39mlist(\n\u001b[1;32m     25\u001b[0m     part\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid,snippet\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvideo\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m     pageToken\u001b[38;5;241m=\u001b[39mnext_page_token  \u001b[38;5;66;03m# Use the next page token for pagination\u001b[39;00m\n\u001b[1;32m     31\u001b[0m )\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Execute the search request and get the response\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m search_response \u001b[38;5;241m=\u001b[39m \u001b[43msearch_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Extract video IDs and publication dates from the search results\u001b[39;00m\n\u001b[1;32m     37\u001b[0m video_ids \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dev/lib/python3.10/site-packages/googleapiclient/_helpers.py:130\u001b[0m, in \u001b[0;36mpositional.<locals>.positional_decorator.<locals>.positional_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m positional_parameters_enforcement \u001b[38;5;241m==\u001b[39m POSITIONAL_WARNING:\n\u001b[1;32m    129\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(message)\n\u001b[0;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dev/lib/python3.10/site-packages/googleapiclient/http.py:938\u001b[0m, in \u001b[0;36mHttpRequest.execute\u001b[0;34m(self, http, num_retries)\u001b[0m\n\u001b[1;32m    936\u001b[0m     callback(resp)\n\u001b[1;32m    937\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[0;32m--> 938\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HttpError(resp, content, uri\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muri)\n\u001b[1;32m    939\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostproc(resp, content)\n",
      "\u001b[0;31mHttpError\u001b[0m: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/search?part=id%2Csnippet&type=video&publishedAfter=2021-01-01T00%3A00%3A00Z&publishedBefore=2022-01-01T00%3A00%3A00Z&maxResults=50&pageToken=CGQQAA&key=AIzaSyCPsdodulfCK_q0DtSoTbq6ybocTWJkklg&alt=json returned \"The request cannot be completed because you have exceeded your <a href=\"/youtube/v3/getting-started#quota\">quota</a>.\". Details: \"[{'message': 'The request cannot be completed because you have exceeded your <a href=\"/youtube/v3/getting-started#quota\">quota</a>.', 'domain': 'youtube.quota', 'reason': 'quotaExceeded'}]\">"
     ]
    }
   ],
   "source": [
    "dataframes = []\n",
    "\n",
    "# Retrieve 500 videos from each year in the specified range and convert to DataFrame\n",
    "for year in years:\n",
    "    videos = get_videos_from_year(year, 500)\n",
    "    # Convert the list of videos to a pandas DataFrame\n",
    "    df = pd.DataFrame(videos)\n",
    "    # Append the DataFrame to the list\n",
    "    dataframes.append(df)\n",
    "    print(f'Fetched {len(df)} videos for the year {year}')\n",
    "\n",
    "# Concatenate all DataFrames into one DataFrame\n",
    "combined_df = pd.concat(dataframes)\n",
    "\n",
    "# Export the combined DataFrame to a single CSV file\n",
    "combined_df.to_csv('all_videos.csv', index=False)\n",
    "print('All videos exported to all_videos.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3192dff6-0fe2-4546-9530-0bbabcc9c9db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
